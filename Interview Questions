What is jenkins remote access API?
  - install the plugin called "Build authorization token trigger"
  - on jenkins at the build triggers enable the option "build this job remotely"
  - demo url "http://localhost:8080/view/CodeBook/job/Job1/build?token=12345"

what are the uses of parameters and how to do build parameterized ?
  - If your application needs to be deployed to different environments, you can use parameters to specify the target environment. 
  - Parameters can be used for specifying version numbers or tags dynamically. 
  - A build parameter allows us to pass data into our Jenkins jobs.
  - Using build parameters, we can pass any data we want: git branch name, secret credentials, hostnames and ports, and so on.
  - Any Jenkins job or pipeline can be parameterized. All we need to do is check the box on the General settings tab, “This project is parameterized”.
  Types of parameters:- Boolean parameter, string parameter, choice parameter, credentials parameter, file parameter, Git parameter.
  
 what are the environment variables in jenkins?
  - Environment variables are predetermined values, Environment variables are global key-value pairs Jenkins can access and inject into a project. 
  - Use Jenkins environment variables to avoid having to code the same values for each project
    BUILD_ID, BUILD_NUMBER, BUILD_URL, JOB_NAME, JENKINS_HOME, WORKSPACE.
  
what is git cherry pick command ?
  - Cherry-picking in Git stands for applying some commit from one branch into another branch.
  - In case you made a mistake and committed a change into the wrong branch, but do not want to merge the whole branch. 
  - You can revert the commit and apply it on another branch.
  - Git cherry-pick is helpful to apply the changes that are accidentally made in the wrong branch. 
  - To make all the changes of the new branch into the master branch, we will use the git pull, but for this particular commit, we will use git cherry-pick command. 
  - $  git cherry-pick <commit id>  
  
How to check the git commit id's ?
  $ git log
  
what is bare metal?
 - Bare metal is a computer system without a base operating system (OS) or installed applications.
 
what is Hypervisor ?
  - A hypervisor is a form of virtualization software used in Cloud hosting to divide and allocate the resources on various pieces of hardware.
  - The program which provides partitioning, isolation, or abstraction is called a virtualization hypervisor.
  - The hypervisor is a hardware virtualization technique that allows multiple guest operating systems (OS) to run on a single host system at the same time.
  - The hypervisor runs directly on the underlying host system. 
  
what is the difference between docker and virtual machine ?
 - virtual machine:- 
     - infrastructure -> Host OS -> Hypervisor -> Guest OS on top of this application will be istalled.
     - Allows multiple virtual machines and applications to run on the same physical machine.
     - Host OS can be different from guest OS.
     
 - Docker:- 
     - infrastructure -> Host OS -> Docker daemon -> application
     - The OS is abstracted via containers rather than the hardware. 
     - With the use of containers, a developer can package up a program with all of its constituent elements, 
       including libraries and other dependencies, and deliver it as a single package. 
       
       
 what is git runner ?
   - GitHub offers hosted virtual machines to run workflows.
   - The virtual machine contains an environment of tools, packages, and settings available for GitHub Actions to use.
   - For example, a runner can clone your repository locally, install testing software, and then run commands that evaluate your code.
   - GitHub provides runners that you can use to run your jobs, or you can host your own runners.
   - Each GitHub-hosted runner is a new virtual machine (VM) hosted by GitHub with the runner application,
     and other tools preinstalled, and is available with Ubuntu Linux, Windows, or macOS operating systems.
   - When the job begins, GitHub automatically provisions a new VM for that job. All steps in the job execute on the VM, 
     allowing the steps in that job to share information using the runner's filesystem.
     
 What is S3 bucket versioning and how can you demonstrate this ?
   :- click on the bucket -> properties -> bucket versioning enable -> enable versioning
   :- Suppose you have hosted static website with the image, after that you have added new image then the old image is not going to replace
      it is just going to create the new version of an object
   :- once you hide versions and delete the image, it won't going to be deleted it's just going to mark as delete marker, and image will not be available.
   :- once you show the versions, it will show delete marker and image versions will be available
   :- if you want rollback to previous versions, you can click on show versions and delete the latest version.
   
 2nd interview - (HITACHI)
 
  1. Explain about S3 lifecycle and how can you move from one storage class to another?
     - click on the bucket - management -> create lifecycle rule -> select the option ( This rule applies to all objects in the bucket )
     - To filter a rule by object size, you can check Specify minimum object size, Specify maximum object size, or both options
     - Under Lifecycle rule actions, choose the actions that you want your lifecycle rule to perform:
        Transition current versions of objects between storage classes
        Transition previous versions of objects between storage classes
        Expire current versions of objects 
         - This configuration allows users to automatically expire the current version of the Amazon S3 objects stored 
           within the bucket after a specified number of days
         - for version eabled buckets, amazon s3 adds a delete marker as current version of an object will be retained as previous version.
           for non current version S3 will premanently removes the object. 
       Permanently delete previous versions of objects
       Storage classes
        Standard-IA
        Intelligent-Tiering
        One Zone-IA
        S3 Glacier Flexible Retrieval
        Glacier Deep Archive
        Add transition to and select the storage class and Days after object creation, enter the number of days after creation to transition the object
        
  2. How can you access the S3 bucket within the VPC?
      Create one public server and private server attach S3 roles to both of the server, after that go to the VPC endpoints there you select
      the S3 gateway an create it, after you creating that entrypoint will be attached to routing table, then you will be able to access to the
      S3 bucket through private server.
      Can be approached through NAT gateway but NAT gateway do cost every hour and traffic is leaving from VPC and also leaving 
      the AWS cloud and then it's coming back all the way back, not a good idea from security perspective.
  
  3. we have multiple users only one person can access to S3 bucket, how can you do that?
  
  4. explin about what is the difference between NAT GW and Internet GW and how can you configure them?
     If any private server wants to communicate with the internet, then we have to create NAT gateway have to communicate
     with the internet through NAT gateway.
     create NAT gateway on public subnet and allocate the elastic IP. in in routing table, click routers and add that NGW
     
     we can create an internet gateway and attach it to the VPC and in routing table, click routers and add that IGW.
     Internet is used to commuinate with the network for public servers.
     
  5. How the frontend web server can communicate with database server?
  
  6. How the 2 VPCs can communicate each other from 2 different accounts?
     Establish the connection between two VPCs through VPC peering.
    
  7. Whenever any instance state gets changed we need get notified how can you configure that?
     create SNS topic and subscription
     go to the cloudwatch -> click on events, select the service as EC2 and choose the state.

  8. How can you access RDS instance through EC2 instance and fetch the information which is stored in RDS?
     first you need to install mysql client on linux machine
     $ sudo yum install mysql 
     allow the 3306 port number on RDS SG and Instance SG
     $ mysql -h endpoint -P port -u masteruser -p 
     
  9. Explain about API Gateway how did you use in your project?
  
  10. what are the limitations of lambda and explain about lambda?
      The disk space (ephemeral) is limited to 512 MB.
      The default deployment package size is 50 MB.
      The memory range is from 128 to 3008 MB.
      The maximum execution timeout for a function is 15 minutes*
      Request and response (synchronous calls) body payload size can be up to to 6 MB.
      Event request (asynchronous calls) body can be up to 128 KB.
      
  11. what are the DNS records ?
       A – maps a hostname to IPv4
       AAAA – maps a hostname to IPv6
       CNAME – maps a hostname to another hostname
       
  12. what is the difference between ALB and NLB?
      <== Application Load Balancer ==>
    • Application load balancers is Layer 7 (HTTP)
    • Load balancing to multiple HTTP applications across machines 
      (target groups)
    • Routing based on path in URL (example.com/users & example.com/posts)
    • Routing based on hostname in URL (one.example.com & other.example.com)
    • ALB are a great fit for micro services & container-based application 
      (example: Docker & Amazon ECS)
    • ALB can route to multiple target groups
    • Health checks are at the target group level
    
 <= Network Load Balancer ==>
    • Network load balancers (Layer 4)
    • Forward TCP & UDP traffic to your instances
    • Handle millions of request per seconds
    • NLB are used for extreme performance, TCP or UDP traffic
    
  13. what could be the reasons of not able to SSH into EC2 instance?
       1. SSH 22 port might not be allowed in security groups
       2. public ip not assigned to that instance
       3. public subnets not been associated with the routing table.
  
  14. I have one server on production env, i have lost the pem.key so how can i login to that instance?
      take the snapshot of the EBS volume, and create volume through snapshot and take snapshot of the AMI,
      whille creating new instance, creat through AMI that we took snapshot from existing VM and attach volume the new instance.
      
  15. what are the difference ways to login to instance apart from SSH?
      SSH into the server and SSH key based authentication.
      
  16. which file will be having all the user information ?
       /etc/passwd ( usernames, encrypted passwords, user id, users group id, the /home directory of the user, users login shell )
       
  17. which file you need to check that how may users have access to the servers?
      
  18. what proc dir will be containing?
      - It contains useful information about the processes that are currently running, it is regarded as control and information center for kernel.
      - ls -ltr /proc/7494
      - Now check the highlighted process with PID=7494, you can check that there is entry for this process in /proc file system.
      - /proc/PID/stat -> Process status
      - /proc/PID/statm -> Process memory status information.
      - /proc/PID/mem	 -> Memory held by this process.
  
  19. am on the the root user and i need to do some tasks with the particular user, how can i do that?
      sudo command -> Run command as root.
      sudo -u root command -> Run command as root.
      sudo -u user command -> Run command as user.
      sudo su -> Switch to the superuser account.
      sudo su -  -> Switch to the superuser account with root's environment.
      sudo su - username -> Switch to the username's account with the username's environment.
      sudo -s -> Start a shell as root
      sudo -u root -s -> Same as above.
      sudo -u user -s -> Start a shell as user.
      
  20. explain about the terraform modules ?
      
  21. you have written terraform script to provision s3 bucket and ec2 instance and subnets, i need to create only ec2 instance how can i do that?
      call the terraform ec2 modue in main.tf file.
      
  22. explain about META arguements in terraform?
      for_each
      The for_each meta-argument accepts a map or a set of strings, and creates an instance for each item in that map or set. 
      resource "google_compute_instance" "vm" {
 
for_each = {
"vm1" = { vm_size = "e2-small", zone = "us-central1-a" }
"vm2" = { vm_size = "e2-medium", zone = "us-central1-b" }
"vm3" = { vm_size = "f1-micro", zone = "us-central1-c" }
}
 
name = each.key
machine_type = each.value.vm_size
zone = each.value.zone 
}
  
  resource "aws_iam_user" "the-accounts" {
  for_each = toset( ["Todd", "James", "Alice", "Dottie"] )
  name     = each.key
}

depends_on
 The "depends_on" meta argument in Terraform is used to specify dependencies between resources within a Terraform configuration.
 
 The Amazon EC2 Instance depends on the Amazon S3 bucket, so it needs to be created the bucket before creating the EC2 instance. 
 
 # main.tf 

#Step 1 - Create S3 Bucket first
resource "aws_s3_bucket" "example_bucket" {
  bucket = "example-bucket"
}

#Step 2 - Create EC2 instance after S3 Bucket
resource "aws_instance" "example_instance" {
  ami           = "ami-0c94855ba95c71c99"
  instance_type = "t2.micro"

  # Define the dependency on S3 Bucket before creating EC2
  depends_on = [aws_s3_bucket.example_bucket]
}

Example:- 2
resource "aws_s3_bucket" "example_bucket" {
  bucket = "example-bucket"
}

# Step 2 - Create an AWS DB Instance
resource "aws_db_instance" "example_db" {
  engine = "mysql"
  instance_class = "db.t2.micro"
}

# Step 3 - Create an EC2 Instance with dependency on S3 Bucket, DB Instance
resource "aws_instance" "example_instance" {
  ami           = "ami-0c94855ba95c71c99"
  instance_type = "t2.micro"

  depends_on = [aws_s3_bucket.example_bucket, aws_db_instance.example_db]
}

Count :- 
  resource "aws_instance" "server" {
  count = 4 # create four similar EC2 instances

  ami           = "ami-a1b2c3d4"
  instance_type = "t2.micro"

  tags = {
    Name = "Server ${count.index}"
  }
}

Provider :-

The provider meta-argument specifies which provider configuration to use for a resource.

  23. you have provision the 5 instance and only 1 instance i need to destroy how can i do that?
        $ terraform state list
        $ terraform destroy -target RESOURCE_TYPE.NAME
        $ terraform destroy -target RESOURCE_TYPE.NAME -target RESOURCE_TYPE2.NAME
      How to delete all resources except one?
        terraform state rm <resource_to_be_deleted> 
        
  24. How can you attach role to the existing ec2 instance.
      From the Actions menu, choose Instance Settings → Attach/Replace IAM role.
      
  25. you have written code for the ec2 instance in different file and how can you define that in main.tf file?
  
      module "ec2-instance_example_complete" {
  source  = "terraform-aws-modules/ec2"
  version = "4.3.0"
}
  
  26. How can you store the secrets in terraform?
      
  
  27. There is a file only you can modify that file linux, how can you do that?
      change the ownership of the file
      $ sudo chown someuser:selectgroup /the/file 
      
  28. explain about git rebase?
      - Rebasing is the process of moving or combining a sequence of commits to a new base commit.
      - The primary reason for rebasing is to maintain a linear project history.  
  
  29. i have made the changes on one branch later i got to know i have done the changes on wrong branch, how can i made changes on new branch?
      if you have already commited then hit the command to reset 
      undo and commit to new branch
      $ git reset HEAD~1
      Move Commits to the Other Branch
      $ git log
      $ git reset --hard HEAD~1
      $ git checkout -b newbranch
      $ git reset --hard < commit_id >
      if you have not commited to and move the changes to other baranch
      $ git stash
      $ git checkout new_branch
      $ git stash pop   
      $ git rebase -i HEAD~5 -> sqush commits into one
      $ git reset --hard HEAD~1 -> to delete the commit
      
  30. what is container orchestration ?
      Container orchestration automates the deployment, management, scaling, and networking of containers.
      
  31. how the two containers will communicate each other and how can you ping them from one container to another container?
      create the docker container in the same network.
      $ docker run -d --name container1 -p 8001:80 nginx
      $ docker run -d --name container2 -p 8002:80 nginx
      $ docker network create mynetwork
      $ docker network connect mynetwork container1
      $ docker network connect mynetwork container2
      $ docker network inspect myNetwork
      $ docker exec -ti web1 ping web2
      
  
  33. what are the networks are available on docker and how can you differentiate them?
      - Dockerhost will be having different subnet and container will be having differenet subnet, whenever you create a container
        a bridge network will automatically create it makes the communication between dockerhost and container, bridge network can be called as
        docker0.
      Host network:- when you create a container it will directly bind to the host network like, whatever dockerhost is having ip address same rang
        of ip address range will get to container. whoever access to host can have access to container as well. this is having some security concerns.
      
  34. How can you upgrade latest image to the container?
      pull the latest image, create container out of it and remove the old pod
      
  35. what is the difference between kubernetes and docker?
      docker is the containarized platform, kubernetes is the container orchestration
      
  36. how can you do autoscaling in kubernetes?
      HorizontalPodAutoscaler controls the scale of a Deployment and its ReplicaSet
      in manifest file have to add the averageUtilization: 70 parametes
      on every node Cadvisor or container adviser runs by google when the pods are running on worker node this C advisor can scrape the pod
      CPU and Memory usage of every 10 sec and every minute metric server will aggregate those metrics and expose them to the kubernetes
      API server and horizontal pod autoscale controller queries the API server for every 15 sec for these metrics once this controller gets
      the desired metrics then it decided scale up or scale down. this HPA controller just updates the replica count in the target deployment
      spinning up the new pod or deleting the existing pod will be taken care by replication controller.
      $ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
      Verticalpodautoscaler performs three steps first it reads the resource metrics of our deployment from metric server based on these metrics
      it recommends the resource metrics and updates to the deployment
      ClusterAutoscaler:- The Cluster Autoscaler automatically adds or removes nodes in a cluster based on resource requests from pods.
      The Cluster Autoscaler doesn’t directly measure CPU and memory usage values to make a scaling decision. 
      Instead, it checks every 10 seconds to detect any pods in a pending state, suggesting that the scheduler 
      could not assign them to a node due to insufficient cluster capacity.
      If there are any pending pods and the cluster needs more resources, CA will extend the cluster by launching a new node
      Kubernetes registers the newly provisioned node with the control plane to make it available to the Kubernetes scheduler for assigning pods.
     Finally, the Kubernetes scheduler allocates the pending pods to the new node.

      
  37. what are the kubernetes services?
      ClusterIP. Exposes a service which is only accessible from within the cluster.
      NodePort. Exposes a service via a static port on each node’s IP.
      LoadBalancer. Exposes the service via the cloud provider’s load balancer.
      
  38. how the communication happen from one pod to another pod in kubernetes
      every pod will be having clusterIP, through clusterIP pods communication will happend
  39. How can you differentiate the containers in pod and how the containers communicate through one container to another container
      pod is a isolated network namespace, container can talk via localhost and port
  40. How can you secure your kubernetes application ?
      RBAC and 
  41. what is kube proxy and kubelet?
      kubelet - An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.
  
  kube-proxy - kube-proxy is a network proxy that runs on each node in your cluster, kube-proxy maintains network rules on nodes. 
    These network rules allow network communication to your Pods
    
  42. How the traffic will go to the pod, tell me the flow and explain?
      traffic will hit to the ingress, and service called load balancer, goes to the Pod.
      
  43. what is container resource monitoring?
      Container monitoring is the activity of continuously collecting metrics and tracking the health of containerized applications and microservices environments, 
      in order to improve their health and performance and ensure they are operating smoothly. 
  44. How can you optimize the kubernetes  
  
3rd interview ( KANINI )
1. explain about docker architecture ?
   - Docker uses a client-server architecture. The Docker client talks to the Docker daemon, 
   - which does the heavy lifting of building, running, and distributing your Docker containers.
   - The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon.
   The Docker daemon
   - The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. 
   The Docker client
   - The Docker client (docker) is the primary way that many Docker users interact with Docker. 
   - When you use commands such as docker run, the client sends these commands to dockerd, which carries them out.
   - The docker command uses the Docker API. The Docker client can communicate with more than one daemon.
   
2. explain about docker daemon
   - The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.
   
3. explain about S3 lifecycle policy?
   - click on the bucket - management -> create lifecycle rule -> select the option ( This rule applies to all objects in the bucket )
     - To filter a rule by object size, you can check Specify minimum object size, Specify maximum object size, or both options
     - Under Lifecycle rule actions, choose the actions that you want your lifecycle rule to perform:
        Transition current versions of objects between storage classes
        Transition previous versions of objects between storage classes
        Expire current versions of objects 
         - This configuration allows users to automatically expire the current version of the Amazon S3 objects stored 
           within the bucket after a specified number of days
         - for version eabled buckets, amazon s3 adds a delete marker as current version of an object will be retained as previous version.
           for non current version S3 will premanently removes the object. 
       Permanently delete previous versions of objects
       Storage classes
        Standard-IA
        Intelligent-Tiering
        One Zone-IA
        S3 Glacier Flexible Retrieval
        Glacier Deep Archive
        Add transition to and select the storage class and Days after object creation, enter the number of days after creation to transition the object
        
4. what is VPC peering and how can you configure that?
    
5. How many S3 buckets can be created per one account?
   100 S3 buckets
   
6. what is blue green deployment strategy ?
   - Blue-green deployment is a technique that reduces downtime and risk by running two identical production environments called Blue and Green.
   - At any time, only one of the environments is live, with the live environment serving all production traffic. 
     For this example, Blue is currently live and Green is idle
   - As you prepare a new version of your software, deployment and the final stage of testing takes place in the environment that is not live:
   - in this example, Green. Once you have deployed and fully tested the software in Green, you switch the router so all incoming requests 
     now go to Green instead of Blue. Green is now live, and Blue is idle.
   - This technique can eliminate downtime due to app deployment. In addition, blue-green deployment reduces risk:
   - if something unexpected happens with your new version on Green, you can immediately roll back to the last version by switching back to Blue.

7. what is stateless and statefull pods in kubernetes?
   which pod is having volume attached to it that is stateful container, which is not having volume attached to it that is stateless.
   
8. what is git fetch ?
   - only downloads latest changes into the local repo. it downloads fresh changes that other developers have pushed to the remote repo.
     it does not merge latest changes to your working dir.
     
9. if my origin origin 2 commits beyond the master how to do that?

10. explain about the Dockerfile structure ?
    FROM centos
    RUN yum install telnet -y && useradd anil  => to run the command inside the container
    ENV MYCOURSE=DevOps  => to export the env variables
    ADD mytarfile.tar /root/  => untar and copy the file
    COPY mytarfile.tr /root/ => only copy the file
    USER anil  => run an image with the user anil
    EXPOSE 80  => to expose the container on port 80 
    WORKDIR /root/mydir  => to specify the working dir in container
    ENTRYPOINT ["/bin/bash", "/tmp/script1.sh"]  
    CMD ["/bin/bash", "/tmp/script1.sh"]
    
11. what is the difference between CMD and ENTRYPOINT ?
    CMD
      vi Dockerfile
      FROM centos
      CMD [ "yum", "-y", "install", "git"
      $ docker build -t demobox .
      $ docker run demobox yum -y install httpd
      - CMD command can override git with httpd while executing container
      CMD [ "yum", "-y", "install", "httpd" ]
      CMD [ "echo", "Anil" ] 
      - only last command will get executed, it will print only Anil i wil not install httpd 
    
    ENTRYPOINT
      vi Dockerfile
      FROM centos
      CMD [ "yum", "-y", "install", "git"
      $ docker build -t demobox .
      $ docker run demobox yum -y install httpd
      - ENTRYPOINT command won't override i will append to the existing command
        i will install both git and httpd.
      ENTRYPOINT [ "yum", "-y", "install", "httpd" ]
      ENTRYPOINT [ "echo", "Anil" ]
      - only last instruction gets executed
      ENTRYPOINT [ "yum", "-y", "install", "httpd" ]
      ENTRYPOINT [ "echo", "Anil" ]
      $ docker run --entrypoint useradd demobox anil
      it will update the entrypoint whatever has been mentioned in the entrypoint that won't be executed.
      
    12. explain about the docker compose ?
    Docker Compose is a tool that was developed to help define and share multi-container applications. 
   $ docker compose build
   $ docker compose up
   $ docker compose up -d
    
    services:
  app:
    image: node:18-alpine
    command: sh -c "yarn install && yarn run dev"
    ports:
      - 3000:3000
    working_dir: /app
    volumes:
      - ./:/app
    environment:
      MYSQL_HOST: mysql
      MYSQL_USER: root
      MYSQL_PASSWORD: secret
      MYSQL_DB: todos

  mysql:
    image: mysql:8.0
    volumes:
      - todo-mysql-data:/var/lib/mysql
    environment:
      MYSQL_ROOT_PASSWORD: secret
      MYSQL_DATABASE: todos

volumes:
  todo-mysql-data:
  
13. what are the different types of jenkins jobs ?
    Maven job, freestyle job, pipeline job.
    
14. what is blue ocean in jenkins ?
    - Blue Ocean is a new user experience for Jenkins based on a personalizable, modern design that allows users to graphically create, 
      visualize and diagnose Continuous Delivery (CD) Pipelines 
    - Alternative options for Pipeline visualization, such as the Pipeline: Stage View and Pipeline Graph View 
    
15. what is poll SCM in Jenkins ?
    - "Poll SCM" polls the SCM periodically for checking if any changes/ new commits were made and shall build the project 
       if any new commits were pushed since the last build.
    - whereas the "build"  shall build the project periodically irrespective to whether or not any changes were made.
    
16. what is multibranch jenkins job?
    - A multi-branch pipeline is a concept of automatically creating Jenkins pipelines based on Git branches. 
    - It can automatically discover new branches in the source control (Github) and automatically create a pipeline for that branch. 
    - When the pipeline build starts, Jenkins uses the Jenkinsfile in that branch for build stages. 
    - scan multibranch pipeline trigger 
    -  multibranch scan webhook trigger plugin to scan the 
    - install the multibranch pipeline plugin and while creating the job  in the branch source select the git and provide repo details
17. what is the lifecycle of multibranch jenkins job?
18. how can you store the credentials in Jenkins?
    Manage jenkins -> configure credentials -> jenkins -> global tool credentials
19. what is the default jenkins dir ?
    /var/lib/jenkins
20. what is the difference between variables and env variables?
21. how to check the syntax error in terraform?
    $ terraform validate 
    
22. How the two containers will communicate each other ?
    create the network and attach same network to two containers
    
23. what is bridge network and how many networks are there in docker ?
    bridge network is the default network in the docker.
    - Dockerhost will be having different subnet and container will be having differenet subnet, whenever you create a container
        a bridge network will automatically create it makes the communication between dockerhost and container, bridge network can be called as
        docker0.
     Host network:- when you create a container it will directly bind to the host network like, whatever dockerhost is having ip address same rang
        of ip address range will get to container. whoever access to host can have access to container as well. this is having some security concerns.
    
24. infrastructure as a service lifecycle.
25. what is the use of main.tf ?
    if we want to call any particular module then we can define in main.tf
    
26. how to view the more information about pod.
    $ kubectl describe pod nginx
    
27. how can you do the kubernetes setup.
28. what is telnet and why to use it ?
    if we can want login to the instance then we should use telnet
    
29. why do you use netstat command ?
    to check the ports
30. what could be the reasons if container is not running up?

31. how to share the information between two tightly coupled containers ?
   using bind mount
   
   $ docker run -it --name container1 nginx /bin/bash
     when you run the container it will creat the docker volume on the host machine under the /var/lib/docker with the name called random has name
     when you delete the container docker volume also will be wiped off.
     
   ananymous volumes
   $ docker run -it --name container1 -v /data01 nginx /bin/bash -> 
     volume will not be having any name, you will get one alpha numeric docker volume name
     /var/lib/docker/volumes/<apha-numeric>/_data/filename
     when you do docker inspect <apha-numeric>, it will not show on which continer it has been mapped
     
  Named volumes
  $ docker volume create volume1
  $ docker run -it --name container1 -v volume1:/data01 nginx /bin/bash
  volume will be mount to the dir called /data01 
  the data will be available Docker host under /var/lib/docker/volumes/volume1/_data/filename
  
  Bind volumes
  $ docker run -it --name container1 -v /opt/data:/data01 nginx /bin/bash
  docker volume will not be created on docker dir called /var/lib/docker/volumes
  /opt/data dir will be created on docker host and it can be shared with multiple containers like NFS
  
4th interview - OpenText
1. write the Dockerfile and explain each innstruction ?
   
2. Explain about the docker networks
3. What is Docker compose, why do we use this ?
4. what is multistage build and why do we use this?
   - multistage builds are used to optimize docket image size
5. what are the security parameters have to take while creating docker image ?
   - always use the docker official images, official images are already built with best practices
   - instead of using all the base images like centos, ubuntu we can use lightweight images
   - lightweight images are not having uneccessary tools and packages are installed which are security issues
   - alpine images are lightweight and secured images
   - optimize caching image layers
   - once image has been built use $ docker scan myapp:1.0 to scan the image, it finds out the vulnarabilitis.
   - when the container starts on the host, it potentially have root access on the docker host, running a application inside the container
     with root user will make it easier for an attacker to escalate privileges on the host
6. which instructions will not be creating layers on Dockerfile.
   Every instruction will make the layer but some layers will not make the size.
   
7. how can you login to the docker container?
    $ docker exec -it <container-name> /bin/bash
    
8. write the declarative pipeline syntax?
9. How to take the backup of the jenkins?
10. what is the default jenkins directory ?
    /var/lib/jenkins
   - it is having information like jobs, plugin, secrets, logs and nodes etc..
11. how can you do the continuoes integration with jenkins to github?
12. Write the terraform script to create the instance ?
13. what is difference between terraform and ansible ?
14. if the state file has been deleted, if am going to do terraform apply it is going to create resources ?
    it's not going to work, we can do the terraform import, terraform plan and terraform apply, then one more state file will be created.
    
15. what is terraform import and why to use that ?
   Write Empty aws_instance resource block- Write an empty terraform resource block for EC2 Instance
   
 provider "aws" {
   region     = "eu-central-1"
   shared_credentials_files = ["/Users/rahulwagh/.aws/credentials"]
 }

 resource "aws_instance" "ec2_example" {
 }

  $ terraform import aws_instance.ec2_example i-097f1ec37854d01c2
  Fill in the resource block- After the successful import add the EC2 resource information into the empty block of aws_instance
   resource "aws_instance" "ec2_example" {
   ami            = "ami-06ce824c157700cd2"
   instance_type  = "t2.micro"
   tags = {
     "Name" = "my-test-ec2"
   }
 }

Now you can verify the terraform import by running the command terraform plan and it should not create any more resources

16. write an ansible playbook to install httpd server
    - name: installing Apache HTTP Server
      hosts: loclahost
      become: true

      tasks:
        - name: install package
          package:
            name: httpd
            state: present

        - name: start package
          service:
            name: httpd
            state: started
            enabled: yes

17. what are the ansible handlers
   - Sometimes you want a task to run only when a change is made on a machine.
   - For example, you may want to restart a service if a task updates the configuration of that service, but not if the configuration is unchanged.
   - Ansible uses handlers to address this use case. Handlers are tasks that only run when notified.
   
   - name: write the apache config file
     ansible.builtin.template:
       src: /srv/httpd.j2
       dest: /etc/httpd.conf
     notify:
     - Restart apache
 handlers:
    - name: Restart apache
      ansible.builtin.service:
        name: httpd
        state: restarted
18. if one task is not working how can you skip that and go to next task ?
    nsible-playbook apache.yaml --skip-tags=install
    
19. what are the ansible tags ?
 ansible-playbook apache.yaml --tags=install,start
 ansible-playbook apache.yaml --skip-tags=install
- If you have a large playbook, it becomes useful to be able to run only a specific part of it rather than running everything in the playbook. 
- hosts: webservers
  become: True
  tasks:
    - name: install packages
      yum:
        name: httpd
        state: present
      tags:
        - install
    - name: start apache server
      service:
         name: httpd
         state: started
         enable: True
      tags:
        - start
    - name: deploy static website
      copy:
        src: index.html
        dest: /var/www/html/
      tags:
        - deploy
20. why do you use ansible galaxy and ansile tower?
21. explain about kubernetes architecture and master and slave components ?
22. what are kubernetes taints and tolerations ?
    in some cases we can't give guarantee that pods cant be scheduled on the nods wich are not having any specific label.
    then we will use the taints and tolerations.
  - Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.
  - Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
  - kubectl taint nodes node1 key1=value1:NoSchedule (pods if they can't tolerate)
  - kubectl taint nodes node1 key1=value1:NoExecute (can be scheduled if no other nodes available)
  - kubectl taint nodes node1 key2=value2:PreferNoSchedule (delete running pods also if they can't tolerate newly added limit)

23. what are the pod affinity and node affinity and what is the difference between selectors?
    NodeSelector:- nodeSelector is the simplest recommended form of node selection constraint
    - You can add the nodeSelector field to your Pod specification and specify the node labels you want
    - the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.
    $ Kubectl label node minicube team=analytics
    $ kubectl label node minicube-03 team=analytics
    - pods will be scheduled wich are having same label as node.
    $ kubectl get nodes --show-labels
    -In manifest file under spec you have to mention nodeSelector andd label.
    
    Node Affinity:-
    - Node Affinity is conceptually similar to nodeselector. it will be having more expressive way and operators.
      requiredDuringSchedulingIgnoredDuringExecution:- while scheduling the pod, pod should have same label otherwise it will sit on the pending state.
      IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.
      operators:- 
        in - it contains
        notin -  it does not contains
        Lt -  less than
        Gt -  greater than
        exist - it is having label
        doesNotExist - it does not having label
        
      preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. 
      If a matching node is not available, the scheduler still schedules the Pod.
      
   Pod Affinity:- 
   - Pod affinity is used to colocate the pods, imagine a case where we want to deploy two applications which communicates freqently in the
     same region that way latency will be less.
     lets consider we have UI application and API application, UI application is deployed on the node us-east-1 region, now we want to deploy
     api application alos in the same region.
     
  podAntiAffinity:-
  - to keep away pods with pod labels topologykey
      
24. whare are the services in kubernetes can you explain each of them ?

            apiVersion: v1
            kind: Service
            metadata:
            name: hello-world
            spec:
            type: NodePort
            selector:
            app: hello-world
            ports:
            - protocol: TCP
            port: 8080
            targetPort: 80
            nodePort: 30036
     Services allow your applications to receive traffic.
     Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service.
     The set of Pods targeted by a Service is usually determined by a label selector
     (see below for why you might want a Service without including a selector in the spec).
     Port:- exposes the Kubernetes service on the specified port within the cluster. 
         Other pods within the cluster can communicate with this server on the specified port.
         
     TargetPort:- is the port on which the service will send requests to, that your pod will be listening on. 
             Your application in the container will need to be listening on this port also.
             
     NodePort:- exposes a service externally to the cluster by means of the target nodes IP address and the NodePort. 
                 external traffic has access to fixed port on each worker node (30000 - 32767).
                 Nodeport services are not secure, you are basically opening the port to directly talk to service on each worker node
                 exter clients basically have access to the worker nodes directly.
                 
    LoadBalancer:- whenever we create loadbalancer service nodeport and clusterip services are creatly automatically, which loadbalancer
                   route the traffic to.
                   This provides an externally-accessible IP address that sends traffic to the correct port on your cluster nodes,
                   provided your cluster runs in a supported environment and is configured with the correct cloud load balancer provider package.
                   
                   $ kubectl expose deployment example --port=8765 --target-port=9376 \
        --name=example-service --type=LoadBalancer            
    $ kubectl get endpoints  => to check which pods are associated with the service
    
25. what is ingress why do we need to use that?
    - suppose we have two services one is for UI and another one is for API so we need to have two load balancer
      have to pay for the two load balancers but we can have one load balancer and we can route to traffic to different
      service through ingress depends upon the path.
    - in ingress we can declare which request should go to which service.
    - Kubernetes ingress resource is responsible for storing DNS routing rules in the cluster.
    - Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. 
    - Traffic routing is controlled by rules defined on the Ingress resource.
    Igress controlller
    $ minikube addons enable ingress -p ingress-cluster
    - ingress controller is a component that reads the ingress rules and process them.
    - whenever we make a request ingress controller inspect http request and direct the request to the correct pods based on the ingress rules that we define.
    - evaluate all the rules
    - manages redirections
    - entrypoint to cluster
    - k8s nginx ingress cotroller
    - Accept traffic from outside the Kubernetes platform, and load balance it to pods (containers) running inside the platform
    - Monitor the pods running in Kubernetes and automatically update the load‑balancing rules when pods are added or removed from a service
    How to configure the SSL certificate?
    - in ingress file have to choose the tls attribute, have to store the certificate in secret that secret have to be mentioned in the 
      ingress file.
      $ minikube addons enable ingress => to create the ingress controller
        
26. what are the security paramameters for creating pod ?
    
27. what is service account in kubernetes ?
    if any application like springboot or python application wants to access the some resourcess.
    service acccounts are nothing but application users, when namespace gets created a default service account will get created
    in the pod definition file have to use the attribute called serviceaccount:test-sa 
    in rolebinding manifest file have to mention the service account.

28. how do you implement RBAC ?
    if we don't have any restrictions there are chances that users my delete the kubernetes resources.
    Role-based access control (RBAC) is a method of regulating access to computer or network resources 
    based on the roles of individual users within your organization.
    Context:- 
    context contains kubernetes cluster, namespace, user
    - first we have to create the private key for the user ( openssl genrsa -out anil.key 2048 )
    - create the certificate sign in request for the user and private key
      ( openssl req -new -key anil.key -out anil.csr -subj "/CN=pavan/O=dev/O=example.org" )
    - this certificate sign in request must be signed by certificate authority.
      openssl x509 -req -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -days 730 -in anil.csr -out anil.crt
    - we can get the certificate authority from kube config file.
    - we should add the user with cluster by providing certificate and user.
      kubectl config set-credentials anil --client-certificate=anil.crt --client-key=anil.key
    -> kubectl config use-context anil-minikube ( to switch the user )
    - after we have to create the manifest file to create the role
    - after that we have to create th rolebinding, rolebinding is nothing but connecting a user with role.
    - role and rolebinding are namespaced, it means user will be having the permission for where rolebinding is defined.
    - cluster rolebinding is at the cluster level, we can access the resources in any namespace   
 
29. What is liveness and readiness probes and why do we use them ?
    Readiness Probe
    - Readiness probes determine whether or not a container is ready to serve requests. 
    - If the readiness probe returns a failed state, then Kubernetes removes the IP address for the container from the endpoints of all Services.
  
   Livenessprobe
   - Liveness probes determine whether or not an application running in a container is in a healthy state. 
   - If the liveness probe detects an unhealthy state, then Kubernetes kills the container and tries to redeploy it.
   
30. what is default kubernetes network ?
    calico
31. what is sidecar deployment ?
    
32. what is the daemonset ?
    A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. 
     As nodes are removed from the cluster, those Pods are garbage collected.
     running a logs collection daemon on every node
     running a node monitoring daemon on every node
     
33. what is the difference betweeen SG and NACL
    - SG serves at the instance level and NACL serves at the subnet level


34. what are the different types of instances ?
    
    
35. what is null resource in terraform ?
    - As in the name you see a prefix null which means this resource will not exist on your Cloud Infrastructure(AWS, Google Cloud, Azure). 
    - The reason is there is no terraform state associated with it.
    
 Ex:-   
 resource "null_resource" "null_resource_simple" {
  provisioner "local-exec" {
    command = "echo Hello World"
  }
}
   
   - The trigger is a block inside the null_resource which holds key-value pair. 
   - As the name suggest trigger, it will execute local-exec, remote-exec or data block.
   - trigger will only work when it detects the change in the key-value pair
   - resource "null_resource" "null_resource_simple" {
  
  # This trigger will only execute once when it detects the instance id of EC2 instance 
  triggers = {
    id = aws_instance.ec2_example.id    # to execute it every time replace - id = time()
  }
  provisioner "local-exec" {
    command = "echo Hello World"
  }
}

1. Run multiple commands- The local-exec provisioner is not limited to running single command but instead, 
   you can run multiple commands within local-exec provisioner using the null_resource.
   
   resource "null_resource" "null_resource_simple" {
  
  triggers = {
    id = aws_instance.ec2_example.id  
  }
  provisioner "local-exec" {
    command = <<-EOT
      chmod +x install-istio.sh  
      ./install-istio.sh
    EOT
  }
}

 -  The remote-exec provisioner will run a script on the remote machine through WinRM or SSH,
 
 resource "aws_instance" "ec2_example" {
  ami           = "ami-0767046d1677be5a0"
  instance_type =  "t2.micro"
  tags = {
    Name = "Terraform EC2 "
  }
}

resource "null_resource" "null_resource_with_remote_exec" {

  triggers = {
    id = aws_instance.ec2_example.id
  }
  
  provisioner "remote-exec" {
    inline = [
      "touch hello.txt",
      "echo helloworld remote provisioner >> hello.txt",
    ]
  }
  
  connection {
    type        = "ssh"
    host        = ec2_example.id
    user        = "ubuntu"
    private_key = file("/home/rahul/Jhooq/keys/aws/aws_key")
    timeout     = "4m"
  }

}


( Avenue E-Commerce )
1. I have a monolithic application running on on-premises, how can i deploy that into microservices?
2. How to transfer the transactions from one container to another container without downtime?
3. Explain about the jenkins parameterized build and why do we use that?
4. Without giving access to the developers to pod, developer can see all the logs collectively, how to give access and how the developers can view the logs?
5. How can you install the RPM on linx OS?
   sudo yum install package_name.rpm

6. Total i have 5 servers running, but i have realized i need to 7 servers so how can you spin up 2 more services through terraform?

( analyttica )

1. what is the difference between NACL and SG and how they work?
2. what is the difference between roles and policies ?
   Policy:-
   - A policy is a document that defines permissions. It specifies what actions are allowed or denied on what resources.
   - Policies are written in JSON format and are attached to entities (users, groups, or roles) in IAM.
   - Policies provide fine-grained control over permissions. You can define policies that grant specific actions on specific resources.
   - Policies are reusable. You can attach the same policy to multiple IAM entities (users, groups, or roles) to grant them the same set of permissions.
   Roles:-
   - A role is an AWS IAM entity that defines a set of permissions for making AWS service requests. 
   -  It is assumed by AWS resources, such as EC2 instances or Lambda functions.
   - Roles enable cross-account access. An IAM user from one AWS account can assume a role in another account 
     if the necessary trust relationships and permissions are configured.

3. what are the storage classes in S3?

4. How can restrict the permissions for S3 bucket and objects ?
   - Attach the custom IAM policy to the IAM user, group, or role that will interact with the S3 bucket.
   - using the bucket policy we can define the who can the bucket and objects
     {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::your-bucket-name/*"
    },
    {
      "Effect": "Deny",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::your-bucket-name/*"
    }
  ]
}

 How can you provide the cross account access for the bucket?
  - click on the bucket, ACL click edit and provide the account ID.
   
5. How to copy file from one linux server to another linux server ?
   scp SourceFile user@server:directory/TargetFile
6. How to check ports in linux server ?
   netstat -tulnp
7. how can you check the background running processors in linux server ?
   ps aux
8. how to check the system performance in linux server ?
   top (  It provides a real-time view of system performance, including CPU usage. )
   free -h ( Displays information about total, used, and free memory. )
   du -f ( Shows disk space usage of a directory. )
   df -h ( Displays information about disk space usage. )
   nload ( Monitors incoming and outgoing traffic separately. )
   
9. How can you make use of the virtual memory in linux ?
10. Can you explain the CI CD workflow in your project ?
11. how to integrate docker with jenkins and build an image ?
12. can you write the Dockerfile to build jenkins image ?
13. what is the difference b/w CMD and ENTRYPOINT ?
14. can you explain about the ansible roles ?
15. can you explain basic syntax for the ansible playbook ?
16. what is ansible inventory file ?
    - In Ansible, an inventory file is a simple text file that defines the hosts and groups 
      of hosts upon which Ansible commands and plays operate.
    ansible -i inventory.ini all -m ping

17. how can you copy the file from dockerhost to docker container ?
    docker cp example.txt my_container:/app/

18. how can you build the docker image ?
    docker build -t myapp:v1.0 -f Dockerfile .

19. how can you check the docker logs ?
    docker logs my_container

20. suppose my container is keep on restarting how can i debug that what would be the reasons ?
21. explain about the docker network ?
22. how the two containers will be communicating each other ?
23. how can you check the memory and cpu utilization of docker container ?
   docker top <container_id_or_name>

24. can you explain about kubernetes architecture ?
25. i want to take one server from the kubernetes cluster for the maintenance, how can i do that?
    kubectl cordon <node_name> ( Mark the node as unschedulable to prevent new pods from being scheduled on it during maintenance )
    Perform the necessary maintenance tasks on the node, such as updates, reboots, or hardware changes.
    After maintenance is complete, mark the node as schedulable again to allow new pods to be scheduled on it:
    kubectl uncordon <node_name>

25. after i done with the maitenance how can i make it available ?
    kubectl uncordon <node_name>

26. can you write the manifest file to deploy the nginx pod ?
27. How you are doing logging for kubernetes logs?
    Prometheus is a monitoring and alerting toolkit. While it primarily focuses on metrics, it can also be used for logging. 
    Grafana can be used for log visualization.
28. what are the different types of services in kubernetes?
29. How can you deploy the pods to the particular nodes ?
    Node Affinity, Node Selector and Tolerations we can use these mechanisms as per the labels can schedule the pods to specific nodes
30. what are the kubernetes taints and tolerations ?
31. how can you define the resource limits for the pods ?
32. 


Ansible version - 2.14
Jenkins version - 2.346
java - 11
Redhat - 7
Kernel version - 5.7
tomcat version - 9
nginx version - 7.4

I have S3 bucket and 500 objects are in standard class and 500 are in infrequent class but I want to move back 500 objects 
which are in infrequent class to the standard class how can i do that?
   S3's transition policies are designed to move objects from Standard to Infrequent Access, Glacier, or Deep Archive,
   but not the other way around.
   Once an object is transitioned to the Infrequent Access class, it stays there until you perform an overwrite operation 
   (like a copy) and specify the storage class as Standard.
   If you need to move objects from Infrequent Access back to Standard, the typical approach involves copying the objects back 
   to the same bucket with the desired storage class and then deleting the original objects in the Infrequent Access class.

what are the difference between RDS multi-az and read replicas?
multi-az:
 - Multi-AZ is primarily focused on enhancing high availability and fault tolerance for a single database instance.
 - With Multi-AZ, you deploy a primary database instance in one Availability Zone, and Amazon RDS automatically provisions
   a synchronous standby replica in another Availability Zone.
 - Multi-AZ operates using synchronous replication, meaning that changes made to the primary instance are immediately replicated 
   to the standby instance. 
Read replica:
 - Read replicas are designed to enhance read scalability by allowing you to create one or more read-only replicas of your 
   primary database instance.
 - Read replicas are separate database instances that are asynchronously replicated from the primary instance. You can have 
   multiple read replicas.
 - Unlike Multi-AZ, read replicas use asynchronous replication, which means there might be a slight lag between the primary
   and replica databases
 - Read replicas are used to offload read traffic from the primary instance, improving overall performance and scalability for 
   read-heavy workloads.


what is the difference between CNAME and alias record in route53?
  CNAME:- points to hostname to hostname
  alias:- points hostname to AWS resource (like points hostname to load balancer endpoint)

Can you explain different types of AWS route53 routing policies?
Geolocation routing:-
  A geolocation routing policy is what you use when you want to route traffic based on the location of your users. 
  A geolocation routing policy allows you to allocate the resources that serve your traffic based on the location 
  that users' DNS queries originate from.
  For example, say you have servers in Oregon and New York, and many of your users are in California. A geolocation routing policy 
  might send those California users to your Oregon server so you can serve them content specific to the West Coast of the US.

Geoproximity Routing Policy:-
  A geoproximity routing policy is what you use when you want to route traffic based on the location of your resources, 
  or shift traffic flow between resources This policy allows you to direct users to different servers, even though those 
  servers might be further away, using something called a bias. Take the previous example. Let's say you have servers in Oregon 
  and New York, and users in California, but your New York servers are larger and can handle more traffic than the Oregon servers.
  You might use a geoproximity routing policy to direct a portion of the California users to the New York server.

Failover routing policy:-
  A failover routing policy allows you to direct users to a particular resource only when that resource is healthy

Latency Routing Policy
  A latency routing policy is what you would use if you have resources in multiple regions and you want to route traffic 
  to the region that provides the best latency. 

Weighted Routing Policy
  A weighted routing policy allows you to send traffic to specific servers, with no consideration for latency or geographic location. 
  You can choose exactly how much traffic is sent to a particular resource, and you have total control over the traffic flow.

How the pods will communicate to the external database servers?
  - Create a Kubernetes Secret to securely store the MySQL connection credentials. 
  - Create a ConfigMap to store the MySQL server host and port information.
  - Modify your application's Kubernetes Deployment configuration to use the created Secret and ConfigMap:
  - In your application code, use the environment variables (MySQL host, port, username, password) to establish 
    a connection to the MySQL server

How the application pod will communicate with database pod?
  - Create a Kubernetes Service for the database pod. The service acts as 
    a stable endpoint that abstracts away the underlying pods
  - In your application pod, you can now use the DNS name of the database service to connect to the database pod. 
    Kubernetes automatically provides DNS resolution for services within the cluste
  - The service discovery works based on the service name and namespace. If your database service is in a different namespace, 
    you might need to use the fully qualified service name (database-service.namespace).
  
Can you explain about the kubernetes network?
  - Every pod has unique IP address that ip address is reachable from all other pods in k8s cluster.
  - you have multiple postgress containers running inside the docker host, then it is difficult to do portmapping on
    each container and which port is still available.
  - docker run -p host_port:container_port image_name
  - docker run -p 8080:80 nginx
  - Then pod comes into picture, pod is a abstraction layer to the container it has ip address and range of ports to allocate
  - suppose you have 10 microservice applications that will run on port 8080 inside 10 different pods and you won't get any
    conflicts because they all are running self contained isolated machines called pods
  - Containers within the same pod share the same network namespace. This implies that they have the same network interfaces 
    and can communicate using localhost
  
what are the kubernetes configmap and secrets and how you have used in your project?
  ConfigMaps:- are used to Storing environment variables and Providing configuration files to applications.
  Enables configuration changes without modifying container images, ConfigMaps are exposed to pods as environment variables 
  or as mounted volumes.
  Note:- Support you have changed the DB port in configmap, that DB port will not be reflected in container, you have to
         create new pod in such cases you need to map configmap to volumes and that volume should be mounted on pod.
  Secrets:- Secrets are intended to store sensitive data like passwords, API keys, and TLS certificates securely.
            Encrypts sensitive data at rest in etcd and Secrets can be mounted into pods as volumes or exposed as environment variables.

what is difference between deployment and statefulsets?

Deployment:-
 - when you create the deployment i will deploy the desired number of pods but when the pod gets restarted again it won't create
   the pod with same name.
 - deployment process will not happen sequentially, if we give the replicas as 3 it starts the process simultaneously to create the
   pods.
 - 2 pods may come into run state after that 1 pod may come up in sometimes one pod may not be deployed.
 - if we want persistant storage we are attaching the PVC to the pod to store the data.

Statefulsets:-
 - it follows the naming convention and sequential order.
 - when we create the pods it creates in particular order like pod-0, pod-1, pod-3.
 - first pod-1 pod should be up and run completely after that pod-1 will start creating.
 - suppose if we update the manifest for 2 replicas statefulset wont delete the pod randomly, it deletes which pod has created last 
   in this case pod-3.
 - statefulsets are commonly used for deploying and managing stateful applications such as mysql, postgress and messaging systems.
 - since it follows consistent network identity it communicates with other pods seamlessly 
 - when you create statefulsets it creates the Headless service which allows each pod to have it's own DNS entry. 

What are the access modes of persistent volumes in k8s?
 ReadWriteOnce:- read write access to one pod and other pods should be having ready only access
 ReadOnlyMany:- all pods are having read only access
 ReadWriteMany:- all pods are having read write access

What are the reclaim policies in PVCs?
 Retain:- with the retain policy the PV associated with he PVC is not automatically deleted or released when the PVC is deleted. instead, the PV is retained
	  and its content are preserved
 Delete:- The delete policy indicates that the associated PV should be automatically deleted when the PVC is deleted.


explain about how ingress and ingress controller will work?
 ingress:-
 An Ingress is a Kubernetes resource that defines rules for external access to services within a cluster. 
 It specifies how incoming HTTP and HTTPS  traffic should be routed to different services based on factors 
 such as the request's host, path, or other attributes.

ingress controller:-
 An Ingress controller, on the other hand, is a component that implements the Ingress resource specification
 by actually managing and fulfilling the routing  rules defined in the Ingress objects.  Ingress controllers 
 dynamically configure routing rules based on changes to Ingress resources. They monitor changes  to Ingress 
 objects and update their routing configurations accordingly.

How to copy the file from remote to local in ansible

---
- name: Copy file from remote to local
  hosts: your_remote_host
  tasks:
    - name: Fetch file from remote machine
      fetch:
        src: /path/to/remote/file.txt
        dest: /path/to/local/directory
        flat: yes


what are the difference between args and env variables in docker
Args:-
 args are used during the image build process to parameterize Dockerfile instructions. They allow you to pass dynamic values to the build process.
 args are only available during the image build process and are not accessible within the running container 
 Example:-
 ARG VERSION=latest
 FROM nginx:${VERSION}

Env:-
 Environment variables are used to configure the behavior of the application running inside the container. They can be used to pass information such as  database connection strings, API keys, or other runtime configuration options.
 Environment variables are available to the running container and can be accessed by the application running inside the container.
 Example:-
 ENV DB_HOST=localhost
 ENV DB_PORT=5432

How to delete the files which are having more disk space
du -ah | sort -rh | head -n 10

what is exception handling in ansible
You can use the ignore_errors directive to ignore specific task failures and continue playbook execution.
- name: This task will not fail the playbook
  command: /path/to/command
  ignore_errors: yes

EPAM
what is the difference between authenticationa and authorization?

Authentication: This is the process of verifying the identity of a user or application that is attempting to connect to an RDS database. Amazon RDS supports multiple authentication methods, including password authentication and IAM database authentication.

Authorization: Once a user or application is authenticated, authorization determines what actions they are allowed to perform on the database. This is typically managed through database-level roles and permissions.

can you use same subnet in another zone which is already used in one zone?
No, you cannot use the same subnet in another Availability Zone (AZ) if it is already used in one zone. Subnets are specific to an AZ and cannot span across multiple AZs within the same region. Each subnet you create must be associated with a single AZ.

what is principle in aws policy?
The Principal can be an AWS account, an IAM user, an IAM role, an AWS service, or an anonymous user (represented by the wildcard "*").

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "s3:ListBucket",
            "Resource": "arn:aws:s3:::example-bucket",
            "Principal": {
                "AWS": "arn:aws:iam::123456789012:user/example-user"
            }
        }
    ]
}


what does asterisk symbol defines in AWS NACL?
In an AWS Network Access Control List (NACL), the asterisk (*) symbol is used as a wildcard to represent all IP addresses (0.0.0.0/0) or all ports (0-65535).
For example, if you configure a rule in a NACL with the source or destination IP address as "", it means that the rule applies to all IP addresses. Similarly, if you configure a rule with the port range as "", it means that the rule applies to all ports.

How to access and read S3 bucket from AWS account to another account?

How to access the container logs that mounted to specific path in container?
- docker logs command with the container ID or name to access the logs
$ docker logs -f <container_id_or_name>
- Alternatively, if you want to access the logs directly from the mounted path on the Docker host, you can navigate to the path where the logs are mounted. 
- By default, Docker mounts volumes under /var/lib/docker/volumes on the host, but the exact path depends on how the volume was mounted.
- You can use the docker inspect command to get more information about the volume and its mount point

can we mount S3 bucket to EC2 instance like we mount EBS volume?
No, you cannot mount an Amazon S3 bucket to an EC2 instance in the same way you mount an Amazon EBS volume. Amazon S3 is an object storage service, while Amazon EBS provides block-level storage volumes for use with EC2 instances.

what is the difference between S3 and EBS volumes?

S3: 
S3 is an object storage service designed for storing and retrieving large amounts of data. It is suitable for storing files, backups, and data sets. Objects in S3 are stored in buckets and accessed over the internet. Object-level storage is a type of storage in which data is stored as objects, each containing the data itself along with metadata and a unique identifier. 

EBS: 
EBS provides block-level storage volumes that can be attached to EC2 instances. It is used for storing data that requires frequent access and low latency, such as operating system files and database storage. EBS volumes are accessed as block devices and can only be attached to one EC2 instance at a time. Block-level storage is a type of storage in which data is stored in fixed-sized blocks, typically ranging from a few bytes to several kilobytes.

what are the different types of EBS volumes are there?
General Purpose SSD (gp2):-
- General Purpose SSD volumes provide a balance of price and performance for a wide variety of workloads. They are suitable for small to medium-sized   databases
Provisioned IOPS SSD (io1):-
- Provisioned IOPS SSD volumes are designed for I/O-intensive workloads, such as large relational databases (e.g., MySQL, PostgreSQL, Oracle) and NoSQL   databases (e.g., MongoDB, Cassandra).
Throughput Optimized HDD (st1):-
- Throughput Optimized HDD volumes are designed for frequently accessed, throughput-intensive workloads, such as big data, data warehouses, and log   processing.
Cold HDD (sc1):-
- Cold HDD volumes are designed for less frequently accessed workloads, where the lowest storage cost is important. Examples include infrequently accessed   file storage and data backups.

what are the lambda layers?
- AWS Lambda layers are a way to centrally manage code and data that is shared across multiple Lambda functions. 
- A Lambda layer is a ZIP archive that contains libraries, custom runtimes, or other dependencies that you want to include in your Lambda functions.
- Share common libraries or dependencies across multiple Lambda functions without including them in the deployment package of each function, reducing   package size and duplication

tell me one example of of each load balancer for application and network load balancer

- Use NLB when you need high-performance, low-latency, and reliable load balancing at the transport layer (Layer 4) of the OSI model.
- NLB is also suitable for TCP and UDP traffic that needs to be distributed across targets, such as in a database or messaging service.

why do we need to use TCP and UDP protocol?

TCP
- TCP is a reliable protocol that ensures that data is delivered in order and without errors. 
- TCP is connection-oriented, meaning it establishes a connection between two hosts before data exchange.
- This ensures that data is delivered accurately and completely 
- TCP is commonly used for applications that require reliable and ordered delivery of data, such as web browsing, email, file transfer (FTP), and database   transactions.

UDP
- UDP, on the other hand, is faster but unreliable. It does not guarantee delivery or order of packets, making it suitable for applications where speed is   more important than reliability.
- UDP is used for applications where real-time communication and speed are more important than reliability, such as VoIP (Voice over IP), online gaming, DNS   (Domain Name System), and streaming media.

terraform script to create AWS instance by using latest ubuntu ami id.

provider "aws" {
  region = "us-west-2" # Set your desired region
}

data "aws_ami" "ubuntu" {
  most_recent = true
  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
  owners = ["099720109477"] # Canonical's account ID
}

resource "aws_instance" "example" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = "t2.micro" # Set your desired instance type
  key_name      = "your-key-pair" # Set your SSH key pair name
}

what is the difference between customer managed policies and inline policy in AWS IAM.

customer managed policies:-
- Customer managed policies are standalone policies that you create and manage independently from any user, group, or role.
- These policies can be attached to multiple IAM identities (users, groups, or roles) across your AWS account.
- When you update a customer managed policy, the changes apply to all the IAM identities to which the policy is attached.
- Customer managed policies are reusable and can help you centralize and manage permissions more efficiently.

Inline Policies:
- Inline policies are policies that are embedded directly into a single IAM identity (user, group, or role) as a JSON document.
- Inline policies are specific to the IAM identity to which they are attached and cannot be reused or attached to other IAM identities.
- When you update an inline policy, the changes apply only to the IAM identity to which the policy is attached.

what is the difference between encryption in transist and encryption at rest?

Encryption in Transit: This refers to the process of encrypting data while it is being transmitted over a network. 
The purpose of encryption in transit is to protect data from being intercepted by unauthorized parties while it 
is being sent from one location to another. This is typically achieved using protocols like TLS (Transport Layer Security) for web traffic

Encryption at Rest: This refers to the process of encrypting data that is stored on a disk or other storage media. 
The purpose of encryption at rest is to protect data from unauthorized access if the storage media is stolen or compromised.

what is SSL/TLS?
- SSL (Secure Sockets Layer) and TLS (Transport Layer Security) are cryptographic protocols that provide secure communication over a computer network.
- TLS is the successor to SSL and is based on the same principles. It was standardized by the Internet Engineering Task Force (IETF) to provide a more   secure and robust protocol for securing communications over the internet.

what is terraform dynamic block?

dynamic blocks allow you to construct nested configuration blocks dynamically based on the elements of a list, map, or set. 
This can be useful when you have a variable number of nested blocks to define, such as when configuring multiple instances in a resource.

Example:-
variable "security_group_rules" {
  type = list(object({
    from_port   = number
    to_port     = number
    protocol    = string
    cidr_blocks = list(string)
  }))
  default = [
    {
      from_port   = 80
      to_port     = 80
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    },
    {
      from_port   = 443
      to_port     = 443
      protocol    = "tcp"
      cidr_blocks = ["0.0.0.0/0"]
    }
  ]
}

resource "aws_security_group" "example" {
  name        = "example"
  description = "Example security group"
  vpc_id      = var.vpc_id

  dynamic "ingress" {
    for_each = var.security_group_rules
    content {
      from_port   = ingress.value.from_port
      to_port     = ingress.value.to_port
      protocol    = ingress.value.protocol
      cidr_blocks = ingress.value.cidr_blocks
    }
  }
}

Example:-2
locals {
  rules = [{
    description = "HTTP",
    port = 80,
    cidr_blocks = ["0.0.0.0/0"],
  },{
    description = "SSH",
    port = 22,
    cidr_blocks = ["10.0.0.0/16"],
  },{
    description = "HTTPS",
    port = 443,
    cidr_blocks = ["0.0.0.0/0"],
  }]
}
resource "aws_security_group" "mysg" {
  name        = "webserver"
  description = "Inbound Rules for WebServer"

  dynamic "ingress" {
    for_each = local.rules
    content {
      description = ingress.value.description
      from_port   = ingress.value.port
      to_port     = ingress.value.port
      protocol    = "tcp"
      cidr_blocks = ingress.value.cidr_blocks
    }
  }
}


How to maintain the multiple accounts in terraform, how can you centralize all these accounts
create AWS profile for each AWS account
[dev]
aws_access_key_id = YOUR_DEV_ACCESS_KEY_ID
aws_secret_access_key = YOUR_DEV_SECRET_ACCESS_KEY

[test]
aws_access_key_id = YOUR_TEST_ACCESS_KEY_ID
aws_secret_access_key = YOUR_TEST_SECRET_ACCESS_KEY

[prod]
aws_access_key_id = YOUR_PROD_ACCESS_KEY_ID
aws_secret_access_key = YOUR_PROD_SECRET_ACCESS_KEY

Define provider aliases for each environment in your Terraform configuration, using the corresponding AWS profile

provider "aws" {
  alias  = "dev"
  region = "us-west-2"
  profile = "dev"
}

provider "aws" {
  alias  = "test"
  region = "us-west-2"
  profile = "test"
}

provider "aws" {
  alias  = "prod"
  region = "us-west-2"
  profile = "prod"
}

Switch between environments by selecting the appropriate provider alias
provider "aws" {
  alias  = "dev"
}

what is static pod?
- Static pods are managed by the kubelet running on the node where they are created.
- They are not scheduled by the Kubernetes scheduler and do not have the flexibility of being rescheduled to other nodes in the cluster if the original node   fails
- kubelet checks the /etc/kubernetes/manifests dir periodically reads the manifest files and create the pods, ensures that pod stay alive and if any changes   happens to the manifests file kubelet recreate the pod
- Since static pods are not managed by the Kubernetes API server, they do not appear in the Kubernetes API and cannot be controlled using kubectl.
- Static pods are often used for critical system components that should run on every node, such as networking plugins (e.g., CNI), monitoring agents, or     logging agents. 











